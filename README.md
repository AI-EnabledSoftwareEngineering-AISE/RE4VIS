# Idias
RE-AIS refers to humans' semantic knowledge of domain concepts to establish a benchmark for assessing the quality of AIS randomly collected training datasets.  Within this framework, a process is implemented to extract common social specifications of primary concepts in a domain, which in turn, derives useful knowledge to be passed to AIS, such that domain specifications are well-met within the intelligent model. RE-AIS automatically creates a set of domain-specific benchmarks, which are further referred to for the purpose of relative evaluation of the correctness and completeness of AIS visual perception. In consequence, AIS data-driven perception, gained through AIS inductive nature, is enhanced with knowledge-based domain analysis. The improvement occurs through the incorporation of inferred knowledge into the AIS training process, exploiting a semantic layout to compensate for the dataset missing variation of a targeted domain concept, and later, to repair AIS misconception of the concept. 

For a fair evaluation of a software product or process, a reference point allows to relatively qualify the comparison, yet, none of the said approaches provided an oracle. Without a reliable point of reference for assessment purposes, the proposal of dataset quality metrics sounds rather abstract, since even assuming that the expected metrics are satisfied (e.g., the dataset is complete), no one could confirm. We aim to use visual representations and techniques to effectively communicate, analyze, and comprehend the requirements and expectations for AI-enabled software. The goal is to connect all components through interactive visualization methods, to ensure the requirements are clear, thorough, properly defined, and aligned with the desired outcomes and constraints of the AI system.

## Design:
Our objective in this project phase is to arrange the information gathered from encyclopedias in a manner that allows us to compare the knowledge derived from this source with that of the image dataset. To achieve this, we have implemented topic modeling to the gathered data, with the aim of uncovering relationships, patterns, and trends within the specifications. By creating a hierarchical structure, we can more accurately represent the connections between the specifications, which will help to clarify any ambiguity and refine the specifications, particularly for complex or difficult-to-specify concepts. To derive a hierarchy of relationships, we utilized a pre-trained transformer-based neural network, BERTopic, to extract higher-level topics and their relationships among related specifications.

To identify topics associated with an image dataset, we are using BLIP, a deep multi-modal model to calculate the embeddings of a multimodal dataset, which includes both images and captions. After generating the embeddings using this model, we feed them into BERTopic modeling techniques based on transformers. Although this technique is commonly used for text data, it can be applied to other data types if embeddings are calculated beforehand. The calculated embeddings, along with other metadata like captions, will then be fed into the topic modeling process. This approach will enable us to cluster image embeddings and generate topic representations based on the associated captions.

## Finding similar topics between image datasets and text documents:
To identify similar topics between an image dataset and text documents, we trained separate BERTopic models on the different modalities, and now we want to explore the similarities between these models. Specifically, we aim to determine if there is any overlap between the topics in the text model and those in the image model. In other words, can we find topics in the text model that are similar to those in the image model?

To achieve this, we compare the topic representations of the two models by examining the c-TF-IDF representations. By analyzing the c-TF-IDF representations, we can determine if there are shared topics or themes between the two modalities. This information can be useful in identifying potential correlations between the image dataset and text documents, and in gaining a better understanding of the relationships between the different topics. Additionally, this analysis can help us identify areas where further research and exploration may be necessary to fully understand the connections between the two modalities.


You can access our visualization tool for [pedestrian](https://observablehq.com/d/0f9f9fd0542c3799) and for [aircraft](https://observablehq.com/d/c286d68ea5b4ee08). Also, there is a variation of our tool which you can use for comparing different concept variants and customize similarity scores. You can access them at these links, [pedestrian](https://observablehq.com/d/6d177cd06f04c14d) and [aircraft](https://observablehq.com/d/e4995a24ba587d64).
